[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Portfolio",
    "section": "",
    "text": "Dengue Cases in Mexico\n\n\n\n\n\n\nData Wrangling\n\n\nR\n\n\nData Visualization\n\n\nTableau\n\n\n\nThis analysis shows the dengue virus cases in Mexico at the start of 2024\n\n\n\n\n\nJun 20, 2024\n\n\nMargarita Valdés A.\n\n\n\n\n\n\n\n\n\n\n\n\nWildfires in Mexico (2024)\n\n\n\n\n\n\nData Visualization\n\n\nTableau\n\n\n\n\n\n\n\n\n\nJun 10, 2024\n\n\nMargarita Valdés A.\n\n\n\n\n\n\n\n\n\n\n\n\nBellabeat Case Study\n\n\n\n\n\n\nData Analysis\n\n\nSQL\n\n\nGoogle Sheets\n\n\n\nThe following analysis was done as a Capstone Project in order to complete the Google Data Analytics Certificate\n\n\n\n\n\nMay 9, 2024\n\n\nMargarita Valdés A.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Bellabeat Case/index.html",
    "href": "posts/Bellabeat Case/index.html",
    "title": "Bellabeat Case Study",
    "section": "",
    "text": "How can a Wellness Technology Company Pay it Smart?\nThe following analysis was performed using BigQuery (SQL) and Connected Sheets (Google Sheets)\n\n\n\n\n\n\n\n1. Summary of business task\nBellabeat was founded in 2014, the company developed one of the first wearables specifically designed for women and has since gone on to create a portfolio of digital products for tracking and improving the health of women. Its focus is on creating innovative health and wellness products for women. The company’s mission is to empower women to take control of their health by providing them with technology-driven solutions that blend design and function.\n\nBusiness task: Analyze smart device data to gain insight into how consumers are using their smart devices. Focus on one of Bellabeat’s products and give insights about what was discovered in the analysis, to help guide the marketing strategy for the company.\n\n\n\n2. Description of the datasets\nThe data used for this analysis was downloaded from the Kaggle page: FitBit Fitness Tracker Data. The data comes from Fitbit users that responded to a survey via Amazon Mechanical Turk. The data was collected from 03.12.2016 to 05.12.2016.\nThe downloaded data came in two folders, one for each month of collected data:\n\nFitabase Data 3.12.16-4.12.16 (11 files)\n\n\ndailyActivity_merged.csv\nheartrate_seconds_merged.csv\nhourlyCalories_merged.csv\nhouryIntensities_merged.csv\nhourlySteps_merged.csv\nminuteCaloriesNarrow_merged.csv\nminuteIntensitiesNarrow_merged.csv\nminuteMETsNarrow_merged.csv\nminuteSleep_merged.csv\nminuteStepsNarrow_merged.csv\nweightLogInfo_merged.csv\n\n\nFitabase Data 4.12.16-5.12.16 (18 files)\n\n\ndailyActivity_merged.csv\ndailyCalories_merged.csv\ndailyIntensities_merged.csv\ndailySteps_merged.csv\nheartrate_seconds_merged.csv\nhourlyCalories_merged.csv\nhouryIntensities_merged.csv\nhourlySteps_merged.csv\nminuteCaloriesNarrow_merged.csv\nminuteCaloriesWide_merged.csv\nminuteIntensitiesNarrow_merged.csv\nminuteIntensitiesWide_merged.csv\nminuteMETsNarrow_merged.csv\nminuteSleep_merged.csv\nminuteStepsNarrow_merged.csv\nminuteStepsWide_merged.csv\nsleepDay_merged.csv\nweightLogInfo_merged.csv\n\n\n\n3. Observations about the data\nThe downloaded data sets were not consistent for both months. There are some extra data sets in the second month that are just the same data in its wide format instead of narrow format. However, there are only data sets with information for daily intensities, daily steps, and sleep day for the second month. After inspection of each dataset I realized that most of the data containing intensities, steps, and calories is already summarized in the dailyActivity_merged.csv data sets. Therefore, I focused on cleaning and analyzing the following datasets:\n\nDaily Activity\nSleep Day\nWeight Log Info\n\nTo understand the collected data I inspected each dataset in SQL to assess how many unique Ids were available per data set and whether they were shared in the other sets.\n\n\n4. Cleaning or manipulation of the data\nBefore I could import the datasets as tables to BigQuery, I cleaned the data using Google Sheets. I searched for duplicates and formatted all the Dates columns since BigQuery did not recognize the date-time format with AM/PM at the end.\nI then proceeded to check for unique Ids in all datasets and to compare if they were shared among them using the following queries:\n\nCount unique Ids in each table:\n\n\nSELECT\n\nCOUNT(DISTINCT Id) AS Tot_id\n\nFROM `[project_name].Bellabeat.[dataset]`\n\nChecked for shared Ids between tables:\n\n\nSELECT\n\nCOUNT(DISTINCT table1.id)\n\nFROM `[project_name].Bellabeat.[dataset1]` AS table1\n\nINNER JOIN `[project_name].Bellabeat.[dataset2]` AS table2\n\nON table1.id = table2.id\nFrom applying the queries above to all tables, I made this summary of the Ids that were shared between datasets to understand how big the sample was, and how I could join the datasets.\n\n\n\n\n\n\n\n\n\nDatasets\nUnique Ids\nShared with daily_activity_3_4\nShared with weightLogInfo_3_4\n\n\ndaily_activity_3_4\n35\n35\n-\n\n\ndaily_activity_4_5\n35\n33\n-\n\n\nSleepDay_4_5\n25\n24\n-\n\n\nweightLogInfo_3_4\n11\n11\n-\n\n\nweightLogInfo_4_5\n8\n8\n6\n\n\n\nThere were only 34 consistent participants that shared data in both of the daily activities data sets. And from the Sleep Day data, only 24. The number of participants is very low, so I decided to merge the data sets including all participants that appeared in both of the daily activity tables. The field names were the same in both sets, so I proceeded to merge them and saved the table in the same project with the following query under the name ‘daily_activity_merged’.\nSELECT *\nFROM `[project_name].Bellabeat.daily_activity_3_4`\nUNION ALL\nSELECT *\nFROM `[project_name].Bellabeat.daily_activity_4_5`\nI then made a summary table including the total and average values of the daily measurements of steps, total distance, sedentary minutes, and burned calories per unique id with the following query, and saved it as ‘daily_activity_summary’:\nSELECT\n id,\n COUNT(ActivityDate) AS active_days,\n SUM(TotalSteps) AS tot_steps,\n AVG(TotalSteps) AS avg_steps,\n SUM(TotalDistance) AS tot_distance,\n AVG(TotalDistance) AS avg_distance,\n SUM(SedentaryMinutes) AS tot_sedentary,\n AVG(SedentaryMinutes) AS avg_sedentary,\n SUM(Calories) AS total_calories,\n AVG(Calories) AS avg_calories\nFROM `[project_name].Bellabeat.daily_activity_merged`\nGROUP BY id\nORDER BY active_days DESC\nFor this analysis, I analyzed two tables, the above table with summary data and a general table including all individual measurements included in the daily_activity, the weight_log, and sleep_day data without grouping by id. To obtain the last mentioned table, I first merged both of the weight_logs tables with the following query and saved it as ‘weight_logs_merged’:\nSELECT *\nFROM `[project_name].Bellabeat.weight_log_3_4`\nUNION ALL\nSELECT *\nFROM `[project_name].Bellabeat.weight_log_4_5`\nThen, I joined all three main tables, ‘daily_activity_merged’, ‘weight_log_merged’, and ‘sleep_day’, joining them by Id and ActivityDate with the following query and named it ‘all_data’:\nSELECT\n table1.*,\n table2.WeightKg,\n table2.WeightPounds,\n table2.BMI,\n table3.TotalMinutesAsleep,\n table3.TotalTimeInBed\nFROM `[project_name].Bellabeat.daily_activity_merged` AS table1\nLEFT JOIN `[project_name].Bellabeat.weight_logs_merged` AS table2\nON table1.Id = table2.Id\n AND table1.ActivityDate = table2.Date\nLEFT JOIN `[project_name].Bellabeat.sleep_day` AS table3\n ON table1.Id = table3.Id\n   AND table1.ActivityDate = table3.Sleep_Day\nORDER BY table1.ActivityDate\nI exported both tables, ‘all_data’ and ‘daily_activity_summary’, to Connected Sheets to further explore, analyze and make visualizations.\n\n\n5. Data Analysis, visualizations and key findings\nI first analyzed how the data was collected during those 62 days and how many unique Ids recorded activities using the table ‘all_data’. I found that the number of recorded activities during the first month (march) were very low, and most users started recording until 01.04.2016 and up to 12-05-2016. There was oddly one day with more than 60 recorded activities, but for the most part, that period it looked even.\n\n\n\n\n\nI then checked for the number of recorded activities for each of the users, or unique Ids. There were around 40 recorded activities on average for the whole sample, and only a few participants recorded significantly less or more activities than the average. It is a small sample to work with, so I decided to leave all participants in, and proceed with the analysis.\n\n\n\n\n\nFrom the data above, I classified each user considering the frequency on which they use their trackers. Participants that used their trackers 15 days or less were considered infrequent users. Participants that used their trackers less than 40 of the 62 days were classified as moderate users. And, participants that used their trackers 40 or more days were considered frequent users. The doughnut chart below, shows that most of the analyzed sample is conformed by moderate and frequent users and the frequent users are 65% of the sample.\n\n\n\n\n\nIn the analyzed data set, the most basic measure that allows us to understand individual activity is the number of steps taken daily. To understand the levels of activity in our sample, and how much they are exercising, I calculated the daily average of total steps taken by the whole group conformed by 35 people, which is a little less than 7,000 steps. Then I checked if there were specific days of the week where they might exercise more or less. During Monday to Friday the number of steps is pretty close to the average, and it looks like people exercise a little more on Saturdays and less on Sundays. This information might help to understand when users might be more likely to actively exercise, and when they might prefer rest. Also, from Monday to Friday, users could be prompted to get more passive exercise by just taking more steps between work, commuting, stairs, etc.\n\n\n\n\n\nSince the analyzed group is small I decided to check for steps taken on different days of the week by each of the individuals. A couple of individuals have recorded steps in only 2-3 days, however those were the ones who only used their trackers for a few days. The rest of the group varied, some showing to record more steps on Fridays, Saturdays, and/ or Sundays. However, differences were not so big when compared to steps taken the rest of the week.\n\n\n\n\n\nI then proceeded to ask if there is a correlation between the measured steps and other measured variables. First, I wanted to know the relationship between daily steps and daily burned calories. The following scatter chart shows a positive relationship. In March of 2020, the NIH recommended to get at least 8,000 daily steps in order to reduce risk of death. However, the average daily steps in our group is below 7,000 steps.\n\n\n\n\n\nI classified individuals in the group, depending on the average number of steps they take per day. Individuals that walk less than 4,000 steps were classified as Not Active. If they take between 4,000 to 8,000 steps, they are classified as Moderate individuals. If they take between 8,000 and 10,000 steps they are classified as Active, and if they track more than 10,000 daily steps on average they are considered Very Active individuals. This classification showed that 60% of the group does not take the 8,000 recommended steps. This data is important to incentivise users to get more steps even in casual daily scenarios, in order to get more daily steps.\n\n\n\n\n\nAs expected, there is also a positive relationship between the amount of Very Active Minutes and Burned Calories. This would be important for users interested in losing weight and setting weekly activity goals. Reminders, to get more active time could help for users that are interested in this goal.\n\n\n\n\n\nFinally, a negative relationship was found between the amount of Sedentary Minutes and the Total Minutes Asleep. Only 24 participants recorded their Sleep activity, and the average amount of Total Minutes Asleep in this analysis is 445 min (7.4 hrs). The CDC recommends that adults sleep &gt;=7 hours a day. This negative relationship indicates that the less active individuals are, the less sleep they get.\n\n\n\n\n\nWhen checking for the average hours of sleep during the week, Sunday and Monday are the most restful nights with 8 hours of sleep. From Tuesday to Saturday there is a reduction in the average sleep. Implementing notifications to incentivise movement during the day, notifications to prepare before sleep, and information about the importance of sleep and sleep hygiene could help users to achieve an ideal amount of daily sleep.\n\n\n\n\n\n\n\n6. Top high-level recommendations\n\nImportant Note: These suggestions are made based on the above described dataset, which consists of a very small sample of 35 individuals from which we do not have information about their gender or age. Also, the data was only collected during two months. Further focusing on a dataset with a bigger sample, preferably focusing on female individuals qu(since that is Bellabeat’s target group) and including more months of data would give more accurate information.\n\n\nOne of the first observations is that not all users are consistent in using their tracker. I would suggest focusing on the positive characteristics of the Bellabeat Ivy+, such as being lightweight, hypoallergenic, discrete and stylish, waterproof, and its long lasting battery. All which makes it a tracker that can be worn daily without needing to take it off to go to bed or to shower, helping to record your daily measurements in a consistent manner and to get more accurate trends and recommendations. \nI would focus on women who might not be primarily focused on getting their exercise stats, but want to achieve a more mindful and balanced life and who like their accessories to reflect their lifestyle and personality.\nFor the Bellabeat app, I would make recommendations to promote ideas for their daily activities. Since most individuals do not get their needed daily 8,000 steps, I would incentivise walking even if it is in more casual scenarios. For people who don’t have time to exercise, promote taking the stairs instead of elevators, walking to nearby places, biking, etc. Since, less sedentarism is related to more (and maybe better) sleep, you can create short info capsules about why being more active during the day can help with having a good night sleep and the importance of sleep and its health benefits. You can also add reminders of when users, according to their preferences, should prepare to go to sleep in order to achieve a good night’s sleep. In general the app should both show individual stats, but also help incentivize the user to be more active giving good suggestions on how to do it."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Margarita’s resume",
    "section": "",
    "text": "JAN 2024 - PRESENT\n\nData Management & Analysis: Led the design, implementation, and maintenance of comprehensive databases for experimental data. Applied data cleaning protocols to ensure data integrity and accuracy. Performed in-depth statistical analysis and created insightful visualizations (using R,  ggplot2) to reveal trends and patterns in complex datasets.\nTraining & Knowledge Sharing: Developed and delivered workshops on statistics, programming in R, and data analysis techniques, empowering colleagues to enhance their research capabilities.\n\n\n\n\nJUN  2020 - NOV  2023\n\nData Collection & Analysis: Designed and managed the implementation of patient surveys, defining key metrics and ensuring data quality. Collaborated with diverse healthcare teams to gather requirements and collect patient samples. Conducted comprehensive analysis of survey and patient sample data, utilizing R (ggplot2, gt) to create informative visualizations that effectively communicated research findings to stakeholders.\nHealth Education: Organized and presented informative talks on the health risks associated with viral diseases to raise awareness among the public.\n\n\n\n\nSEP 2012 - OCT 2018\n\nLarge-Scale Data Analysis: Designed and executed experiments involving large-scale sequencing data analysis of thousands of transcripts across diverse conditions. Employed bioinformatics tools to identify significant changes induced by viral infection, contributing to a deeper understanding of viral mechanisms.\nScientific Communication: Presented research findings at international conferences, showcasing the ability to effectively communicate complex data analysis to a scientific audience.\n\n\n\n\nJAN - MAY  2012\n\nPatent Analysis & Consultation: Provided expert consultation to national and international companies, guiding them through the patent application process under Mexican industrial property law. Conducted in-depth patent analysis within the chemistry, pharmaceuticals, and biotechnology sectors."
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Margarita’s resume",
    "section": "",
    "text": "JAN 2024 - PRESENT\n\nData Management & Analysis: Led the design, implementation, and maintenance of comprehensive databases for experimental data. Applied data cleaning protocols to ensure data integrity and accuracy. Performed in-depth statistical analysis and created insightful visualizations (using R,  ggplot2) to reveal trends and patterns in complex datasets.\nTraining & Knowledge Sharing: Developed and delivered workshops on statistics, programming in R, and data analysis techniques, empowering colleagues to enhance their research capabilities.\n\n\n\n\nJUN  2020 - NOV  2023\n\nData Collection & Analysis: Designed and managed the implementation of patient surveys, defining key metrics and ensuring data quality. Collaborated with diverse healthcare teams to gather requirements and collect patient samples. Conducted comprehensive analysis of survey and patient sample data, utilizing R (ggplot2, gt) to create informative visualizations that effectively communicated research findings to stakeholders.\nHealth Education: Organized and presented informative talks on the health risks associated with viral diseases to raise awareness among the public.\n\n\n\n\nSEP 2012 - OCT 2018\n\nLarge-Scale Data Analysis: Designed and executed experiments involving large-scale sequencing data analysis of thousands of transcripts across diverse conditions. Employed bioinformatics tools to identify significant changes induced by viral infection, contributing to a deeper understanding of viral mechanisms.\nScientific Communication: Presented research findings at international conferences, showcasing the ability to effectively communicate complex data analysis to a scientific audience.\n\n\n\n\nJAN - MAY  2012\n\nPatent Analysis & Consultation: Provided expert consultation to national and international companies, guiding them through the patent application process under Mexican industrial property law. Conducted in-depth patent analysis within the chemistry, pharmaceuticals, and biotechnology sectors."
  },
  {
    "objectID": "resume.html#courses-certifications",
    "href": "resume.html#courses-certifications",
    "title": "Margarita’s resume",
    "section": "Courses & Certifications",
    "text": "Courses & Certifications\nGoogle Data Analytics Professional Certificate | Google, Coursera | 2024\nData Cleaning with R Course | R for the Rest of Us | 2022\nR Programming | Johns Hopkins University, Coursera | 2015"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Margarita’s resume",
    "section": "Education",
    "text": "Education\n\nHamburg University, Germany | Doctoral degree\nDoctor of Philosophy, Virology | Faculty of Mathematics, Informatics and  Natural Sciences.\nJUN 2012 - OCT  2018\n\n\nNational Autonomous University of Mexico, Mexico | Master’s Degree\nMasters in Biochemical Sciences | Biotechnology Institute.\nFEB 2009 - NOV 2011\n\n\nNational Autonomous University of Morelos, Mexico | Bachelor’s Degree\nBachelor of Science in Biochemistry and Molecular Biology | Faculty of Sciences.\nAUG 2004 - DEC 2008"
  },
  {
    "objectID": "posts/Mexico's Wildfires/index.html",
    "href": "posts/Mexico's Wildfires/index.html",
    "title": "Wildfires in Mexico (2024)",
    "section": "",
    "text": "The following Tableau Dashboard was made using the latest data about wildfires in Mexico from the National Concentration of Forest Fires in protected natural areas. The captured data spans from January to the first week of June 2024. To get a better view of the dashboard, click on the “full screen” button in the lower right corner.\nThe data is in Spanish, but it encompasses the number of wildfires per week, the damaged area, the states where the fires occurred in, the type of vegetation damaged by the fires, and possible causes for the fires. The dashboard allows you to filter by each of the variables by clicking, or filtering by a range of weeks on the upper bar."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "My name is Margarita, I am a Data Analyst with a strong foundation in science. I hold a PhD in Molecular Virology and possess experience analyzing large datasets from questionnaires and experimental measurements. During my research, I honed my skills in statistical analysis, data visualization, and problem-solving to tackle complex scientific questions using data. I’m proficient in spreadsheets, SQL, and R. My passion lies in transforming raw data into insights that can drive informed decision-making."
  },
  {
    "objectID": "posts/Dengue cases in México/index.html",
    "href": "posts/Dengue cases in México/index.html",
    "title": "Dengue Cases in Mexico",
    "section": "",
    "text": "Dengue cases in Mexico (2024)\nFor this analysis, the data wrangling was done with R using the {tidyverse} library of packages. To visualize the data I made an interactive dashboard in Tableau.\n\n1) About the data\nThe data for this project was downloaded from the Mexican Health Secretary database webpage. The collected data spans from the 1st of January to the 4th of June 2024. The day cases are recorded are later than the reported date of the start of symptoms, so the monitoring of cases has a lag between the symptoms and when they are reported, which can be several days later.\nThree files were downloaded: 1) the first is a .csv file with the actual dengue data Database of Dengue in Mexico (even thought the title is not updated and it says 2020, after inspecting it, the file does indeed contain the 2024 data). 2) The second and 3) third are .xlsx files, and they are downloaded together in a folder. They contain the Dictionary and Catalog of Dengue. The catalog contains the keys for all the variables in the dengue database, and the dictionary contains the description of the variables.\n\n\n2) Data Wrangling\n\na) Importing the data\nFirst, I imported the “dengue_abierto.csv” file to R Studio as a tibble. I inspected the columns and rows, and found there are 78,184 observations and 28 variables. Most of the variables are coded in keys, which are translated later on.\n\nlibrary(tidyverse)\n(casos &lt;- read_csv(file = \"dengue_abierto.csv\"))\n\n\n  \n\n\n\n\n\nb) Selecting working variables\nI decided which variables I was interested in analyzing and selected those columns, to only focus on that data. The variables I selected contained the: ID, sex, age, state of residency, date of intial symptoms, type of patient, hemorrhagic cases, death cases, if they were sampled, PCR results, and status of the case.\n\n(selected_variables &lt;- casos %&gt;% \n          select(ID_REGISTRO,\n                 SEXO,\n                 EDAD_ANOS,\n                 ENTIDAD_RES,\n                 FECHA_SIGN_SINTOMAS,\n                 TIPO_PACIENTE,\n                 HEMORRAGICOS,\n                 DEFUNCION,\n                 DICTAMEN,\n                 TOMA_MUESTRA,\n                 RESULTADO_PCR,\n                 ESTATUS_CASO))\n\n\n\nc) Re-writing keys\nUsing the file “Catalogos_Dengue.xlsx” I renamed the keys on the working variables I selected, to understand the data set I was working with. As an example, keys (in numbers) were used to declare if a variable was negative or positive, the state of residency, the sex, etc.\n\n(rename_variables &lt;- selected_variables %&gt;% \n          mutate(SEXO = case_when(SEXO == 1 ~ \"Mujer\",\n                                  SEXO == 2 ~ \"Hombre\")) %&gt;% \n          mutate(ENTIDAD_RES = case_when(ENTIDAD_RES == \"01\" ~ \"Aguascalientes\",\n                                    ENTIDAD_RES == \"02\" ~ \"Baja California\",\n                                    ENTIDAD_RES == \"03\" ~ \"Baja California Sur\",\n                                    ENTIDAD_RES == \"04\" ~ \"Campeche\",\n                                    ENTIDAD_RES == \"05\" ~ \"Coahuila\",\n                                    ENTIDAD_RES == \"06\" ~ \"Colima\",\n                                    ENTIDAD_RES == \"07\" ~ \"Chiapas\",\n                                    ENTIDAD_RES == \"08\" ~ \"Chihuahua\",\n                                    ENTIDAD_RES == \"09\" ~ \"Ciudad de México\",\n                                    ENTIDAD_RES == \"10\" ~ \"Durango\",\n                                    ENTIDAD_RES == \"11\" ~ \"Guanajuato\",\n                                    ENTIDAD_RES == \"12\" ~ \"Guerrero\",\n                                    ENTIDAD_RES == \"13\" ~ \"Hidalgo\",\n                                    ENTIDAD_RES == \"14\" ~ \"Jalisco\",\n                                    ENTIDAD_RES == \"15\" ~ \"México\",\n                                    ENTIDAD_RES == \"16\" ~ \"Michoacán\",\n                                    ENTIDAD_RES == \"17\" ~ \"Morelos\",\n                                    ENTIDAD_RES == \"18\" ~ \"Nayarit\",\n                                    ENTIDAD_RES == \"19\" ~ \"Nuevo León\",\n                                    ENTIDAD_RES == \"20\" ~ \"Oaxaca\",\n                                    ENTIDAD_RES == \"21\" ~ \"Puebla\",\n                                    ENTIDAD_RES == \"22\" ~ \"Querétaro\",\n                                    ENTIDAD_RES == \"23\" ~ \"Quintana Roo\",\n                                    ENTIDAD_RES == \"24\" ~ \"San Luis Potosí\",\n                                    ENTIDAD_RES == \"25\" ~ \"Sinaloa\",\n                                    ENTIDAD_RES == \"26\" ~ \"Sonora\",\n                                    ENTIDAD_RES == \"27\" ~ \"Tabasco\",\n                                    ENTIDAD_RES == \"28\" ~ \"Tamaulipas\",\n                                    ENTIDAD_RES == \"29\" ~ \"Tlaxcala\",\n                                    ENTIDAD_RES == \"30\" ~ \"Veracruz\",\n                                    ENTIDAD_RES == \"31\" ~ \"Yucatán\",\n                                    ENTIDAD_RES == \"32\" ~ \"Zacatecas\",\n                                    ENTIDAD_RES == \"33\" ~ \"Otros paises\",\n                                    ENTIDAD_RES == \"34\" ~ \"Otros paises\",\n                                    ENTIDAD_RES == \"35\" ~ \"Otros paises\",\n                                    .default = as.character(ENTIDAD_RES))) %&gt;% \n           mutate(TIPO_PACIENTE = case_when(TIPO_PACIENTE == 1 ~ \"ambulatorio\",\n                                            TIPO_PACIENTE == 2 ~ \"hospitalizado\")) %&gt;% \n           mutate(HEMORRAGICOS = case_when(HEMORRAGICOS == 1 ~ \"hemorragico\",\n                                           HEMORRAGICOS == 2 ~ \"no hemorragico\")) %&gt;% \n           mutate(DEFUNCION = case_when(DEFUNCION == 1 ~ \"si\",\n                                        DEFUNCION == 2 ~ \"no\")) %&gt;% \n           mutate(DICTAMEN = case_when(DICTAMEN == 1 ~ \"dengue\",\n                                       DICTAMEN == 2 ~ \"chikungunya\",\n                                       DICTAMEN == 3 ~ \"negativo\",\n                                       DICTAMEN == 4 ~ \"en estudio\",\n                                       DICTAMEN == 5 ~ \"no aplica\")) %&gt;% \n           mutate(TOMA_MUESTRA = case_when(TOMA_MUESTRA == 1 ~ \"si\",\n                                           TOMA_MUESTRA == 2 ~ \"no\")) %&gt;% \n           mutate(RESULTADO_PCR = case_when(RESULTADO_PCR == 1 ~ \"DENV1\",\n                                            RESULTADO_PCR == 2 ~ \"DENV2\",\n                                            RESULTADO_PCR == 3 ~ \"DENV3\",\n                                            RESULTADO_PCR == 4 ~ \"DENV4\",\n                                            RESULTADO_PCR == 5 ~ \"sin serotipo aislado\")) %&gt;% \n           mutate(ESTATUS_CASO = case_when(ESTATUS_CASO == 1 ~ \"probable\",\n                                           ESTATUS_CASO == 2 ~ \"confirmado\",\n                                           ESTATUS_CASO == 3 ~ \"descartado\"))\n                   \n )\n\n\n  \n\n\n\n\n\nd) Filtering out negative cases\nAs I was only interested in working with the possible positive cases, I filtered the variable containing the “status of the case” to only keep the confirmed and the possible cases, and to filter out negative cases. Afterwards, I saved the cleaned and filterd data as a .csv file.\n\n(positive_cases &lt;- rename_variables %&gt;% \n          filter(ESTATUS_CASO != \"descartado\")) \n\n# write_csv(positive_cases,\n#           file = \"positive_dengue_cases.csv\")\n\n\n\n\n3) Data Visualization\nThe visualization was done in the Tableau Public desktop app. I imported the “positive_dengue_cases.csv” to the app and proceeded to explore the data and finally decided to show the data for daily cases (as a bar chart), the cases per state (as a map), and the isolated serotypes of the dengue virus, the number of hemorrhagic cases divided by sex, and the number of deaths divided on weather they were hospitalized or outpatient.\n\nImportant note: The below shown dates of start of symptoms differ from the day cases are reported and there is a lag of several days between when cases start and when they get reported. Therefore, the effect were cases drop in the last days is due to this lag, not to the actual drop in cases."
  }
]
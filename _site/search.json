[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Portfolio",
    "section": "",
    "text": "Website Activity Dasboard | PowerBI\n\n\n\nData Visualization\n\n\nPowerBI\n\n\nKPIs\n\n\n\nPowerBI dashboard using data from a fictional company called Bicycle Empire\n\n\n\nMargarita Valdés A.\n\n\nSep 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHuman Resources Dashboard | Tableau\n\n\n\nData Visualization\n\n\nTableau\n\n\nPython\n\n\nChatGPT\n\n\n\nTableau dashboard using HR mock data\n\n\n\nMargarita Valdés A.\n\n\nAug 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReports of UFO Sightings | Looker Studio\n\n\n\nData Wrangling\n\n\nGoogle Sheets\n\n\nData Visualization\n\n\nLooker Studio\n\n\n\nThis analysis shows the reported cases of UFO sightings to NUFORC\n\n\n\nMargarita Valdés A.\n\n\nJul 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDengue Cases in Mexico\n\n\n\nData Wrangling\n\n\nR\n\n\nData Visualization\n\n\nTableau\n\n\n\nThis analysis shows the dengue virus cases in Mexico at the start of 2024\n\n\n\nMargarita Valdés A.\n\n\nJun 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWildfires in Mexico (2024)\n\n\n\nData Visualization\n\n\nTableau\n\n\n\n\n\n\n\nMargarita Valdés A.\n\n\nJun 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBellabeat Case Study\n\n\n\nData Analysis\n\n\nSQL\n\n\nGoogle Sheets\n\n\n\nThe following analysis was done as a Capstone Project in order to complete the Google Data Analytics Certificate\n\n\n\nMargarita Valdés A.\n\n\nMay 9, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Bellabeat Case/index.html",
    "href": "posts/Bellabeat Case/index.html",
    "title": "Bellabeat Case Study",
    "section": "",
    "text": "How can a Wellness Technology Company Pay it Smart?\nThe following analysis was performed using BigQuery (SQL) and Connected Sheets (Google Sheets)\n\n\n\n\n\n\n\n1. Summary of business task\nBellabeat was founded in 2014, the company developed one of the first wearables specifically designed for women and has since gone on to create a portfolio of digital products for tracking and improving the health of women. Its focus is on creating innovative health and wellness products for women. The company’s mission is to empower women to take control of their health by providing them with technology-driven solutions that blend design and function.\n\nBusiness task: Analyze smart device data to gain insight into how consumers are using their smart devices. Focus on one of Bellabeat’s products and give insights about what was discovered in the analysis, to help guide the marketing strategy for the company.\n\n\n\n2. Description of the datasets\nThe data used for this analysis was downloaded from the Kaggle page: FitBit Fitness Tracker Data. The data comes from Fitbit users that responded to a survey via Amazon Mechanical Turk. The data was collected from 03.12.2016 to 05.12.2016.\nThe downloaded data came in two folders, one for each month of collected data:\n\nFitabase Data 3.12.16-4.12.16 (11 files)\n\n\ndailyActivity_merged.csv\nheartrate_seconds_merged.csv\nhourlyCalories_merged.csv\nhouryIntensities_merged.csv\nhourlySteps_merged.csv\nminuteCaloriesNarrow_merged.csv\nminuteIntensitiesNarrow_merged.csv\nminuteMETsNarrow_merged.csv\nminuteSleep_merged.csv\nminuteStepsNarrow_merged.csv\nweightLogInfo_merged.csv\n\n\nFitabase Data 4.12.16-5.12.16 (18 files)\n\n\ndailyActivity_merged.csv\ndailyCalories_merged.csv\ndailyIntensities_merged.csv\ndailySteps_merged.csv\nheartrate_seconds_merged.csv\nhourlyCalories_merged.csv\nhouryIntensities_merged.csv\nhourlySteps_merged.csv\nminuteCaloriesNarrow_merged.csv\nminuteCaloriesWide_merged.csv\nminuteIntensitiesNarrow_merged.csv\nminuteIntensitiesWide_merged.csv\nminuteMETsNarrow_merged.csv\nminuteSleep_merged.csv\nminuteStepsNarrow_merged.csv\nminuteStepsWide_merged.csv\nsleepDay_merged.csv\nweightLogInfo_merged.csv\n\n\n\n3. Observations about the data\nThe downloaded data sets were not consistent for both months. There are some extra data sets in the second month that are just the same data in its wide format instead of narrow format. However, there are only data sets with information for daily intensities, daily steps, and sleep day for the second month. After inspection of each dataset I realized that most of the data containing intensities, steps, and calories is already summarized in the dailyActivity_merged.csv data sets. Therefore, I focused on cleaning and analyzing the following datasets:\n\nDaily Activity\nSleep Day\nWeight Log Info\n\nTo understand the collected data I inspected each dataset in SQL to assess how many unique Ids were available per data set and whether they were shared in the other sets.\n\n\n4. Cleaning or manipulation of the data\nBefore I could import the datasets as tables to BigQuery, I cleaned the data using Google Sheets. I searched for duplicates and formatted all the Dates columns since BigQuery did not recognize the date-time format with AM/PM at the end.\nI then proceeded to check for unique Ids in all datasets and to compare if they were shared among them using the following queries:\n\nCount unique Ids in each table:\n\n\nSELECT\n\nCOUNT(DISTINCT Id) AS Tot_id\n\nFROM `[project_name].Bellabeat.[dataset]`\n\nChecked for shared Ids between tables:\n\n\nSELECT\n\nCOUNT(DISTINCT table1.id)\n\nFROM `[project_name].Bellabeat.[dataset1]` AS table1\n\nINNER JOIN `[project_name].Bellabeat.[dataset2]` AS table2\n\nON table1.id = table2.id\nFrom applying the queries above to all tables, I made this summary of the Ids that were shared between datasets to understand how big the sample was, and how I could join the datasets.\n\n\n\n\n\n\n\n\n\nDatasets\nUnique Ids\nShared with daily_activity_3_4\nShared with weightLogInfo_3_4\n\n\ndaily_activity_3_4\n35\n35\n-\n\n\ndaily_activity_4_5\n35\n33\n-\n\n\nSleepDay_4_5\n25\n24\n-\n\n\nweightLogInfo_3_4\n11\n11\n-\n\n\nweightLogInfo_4_5\n8\n8\n6\n\n\n\nThere were only 34 consistent participants that shared data in both of the daily activities data sets. And from the Sleep Day data, only 24. The number of participants is very low, so I decided to merge the data sets including all participants that appeared in both of the daily activity tables. The field names were the same in both sets, so I proceeded to merge them and saved the table in the same project with the following query under the name ‘daily_activity_merged’.\nSELECT *\nFROM `[project_name].Bellabeat.daily_activity_3_4`\nUNION ALL\nSELECT *\nFROM `[project_name].Bellabeat.daily_activity_4_5`\nI then made a summary table including the total and average values of the daily measurements of steps, total distance, sedentary minutes, and burned calories per unique id with the following query, and saved it as ‘daily_activity_summary’:\nSELECT\n id,\n COUNT(ActivityDate) AS active_days,\n SUM(TotalSteps) AS tot_steps,\n AVG(TotalSteps) AS avg_steps,\n SUM(TotalDistance) AS tot_distance,\n AVG(TotalDistance) AS avg_distance,\n SUM(SedentaryMinutes) AS tot_sedentary,\n AVG(SedentaryMinutes) AS avg_sedentary,\n SUM(Calories) AS total_calories,\n AVG(Calories) AS avg_calories\nFROM `[project_name].Bellabeat.daily_activity_merged`\nGROUP BY id\nORDER BY active_days DESC\nFor this analysis, I analyzed two tables, the above table with summary data and a general table including all individual measurements included in the daily_activity, the weight_log, and sleep_day data without grouping by id. To obtain the last mentioned table, I first merged both of the weight_logs tables with the following query and saved it as ‘weight_logs_merged’:\nSELECT *\nFROM `[project_name].Bellabeat.weight_log_3_4`\nUNION ALL\nSELECT *\nFROM `[project_name].Bellabeat.weight_log_4_5`\nThen, I joined all three main tables, ‘daily_activity_merged’, ‘weight_log_merged’, and ‘sleep_day’, joining them by Id and ActivityDate with the following query and named it ‘all_data’:\nSELECT\n table1.*,\n table2.WeightKg,\n table2.WeightPounds,\n table2.BMI,\n table3.TotalMinutesAsleep,\n table3.TotalTimeInBed\nFROM `[project_name].Bellabeat.daily_activity_merged` AS table1\nLEFT JOIN `[project_name].Bellabeat.weight_logs_merged` AS table2\nON table1.Id = table2.Id\n AND table1.ActivityDate = table2.Date\nLEFT JOIN `[project_name].Bellabeat.sleep_day` AS table3\n ON table1.Id = table3.Id\n   AND table1.ActivityDate = table3.Sleep_Day\nORDER BY table1.ActivityDate\nI exported both tables, ‘all_data’ and ‘daily_activity_summary’, to Connected Sheets to further explore, analyze and make visualizations.\n\n\n5. Data Analysis, visualizations and key findings\nI first analyzed how the data was collected during those 62 days and how many unique Ids recorded activities using the table ‘all_data’. I found that the number of recorded activities during the first month (march) were very low, and most users started recording until 01.04.2016 and up to 12-05-2016. There was oddly one day with more than 60 recorded activities, but for the most part, that period it looked even.\n\n\n\n\n\nI then checked for the number of recorded activities for each of the users, or unique Ids. There were around 40 recorded activities on average for the whole sample, and only a few participants recorded significantly less or more activities than the average. It is a small sample to work with, so I decided to leave all participants in, and proceed with the analysis.\n\n\n\n\n\nFrom the data above, I classified each user considering the frequency on which they use their trackers. Participants that used their trackers 15 days or less were considered infrequent users. Participants that used their trackers less than 40 of the 62 days were classified as moderate users. And, participants that used their trackers 40 or more days were considered frequent users. The doughnut chart below, shows that most of the analyzed sample is conformed by moderate and frequent users and the frequent users are 65% of the sample.\n\n\n\n\n\nIn the analyzed data set, the most basic measure that allows us to understand individual activity is the number of steps taken daily. To understand the levels of activity in our sample, and how much they are exercising, I calculated the daily average of total steps taken by the whole group conformed by 35 people, which is a little less than 7,000 steps. Then I checked if there were specific days of the week where they might exercise more or less. During Monday to Friday the number of steps is pretty close to the average, and it looks like people exercise a little more on Saturdays and less on Sundays. This information might help to understand when users might be more likely to actively exercise, and when they might prefer rest. Also, from Monday to Friday, users could be prompted to get more passive exercise by just taking more steps between work, commuting, stairs, etc.\n\n\n\n\n\nSince the analyzed group is small I decided to check for steps taken on different days of the week by each of the individuals. A couple of individuals have recorded steps in only 2-3 days, however those were the ones who only used their trackers for a few days. The rest of the group varied, some showing to record more steps on Fridays, Saturdays, and/ or Sundays. However, differences were not so big when compared to steps taken the rest of the week.\n\n\n\n\n\nI then proceeded to ask if there is a correlation between the measured steps and other measured variables. First, I wanted to know the relationship between daily steps and daily burned calories. The following scatter chart shows a positive relationship. In March of 2020, the NIH recommended to get at least 8,000 daily steps in order to reduce risk of death. However, the average daily steps in our group is below 7,000 steps.\n\n\n\n\n\nI classified individuals in the group, depending on the average number of steps they take per day. Individuals that walk less than 4,000 steps were classified as Not Active. If they take between 4,000 to 8,000 steps, they are classified as Moderate individuals. If they take between 8,000 and 10,000 steps they are classified as Active, and if they track more than 10,000 daily steps on average they are considered Very Active individuals. This classification showed that 60% of the group does not take the 8,000 recommended steps. This data is important to incentivise users to get more steps even in casual daily scenarios, in order to get more daily steps.\n\n\n\n\n\nAs expected, there is also a positive relationship between the amount of Very Active Minutes and Burned Calories. This would be important for users interested in losing weight and setting weekly activity goals. Reminders, to get more active time could help for users that are interested in this goal.\n\n\n\n\n\nFinally, a negative relationship was found between the amount of Sedentary Minutes and the Total Minutes Asleep. Only 24 participants recorded their Sleep activity, and the average amount of Total Minutes Asleep in this analysis is 445 min (7.4 hrs). The CDC recommends that adults sleep &gt;=7 hours a day. This negative relationship indicates that the less active individuals are, the less sleep they get.\n\n\n\n\n\nWhen checking for the average hours of sleep during the week, Sunday and Monday are the most restful nights with 8 hours of sleep. From Tuesday to Saturday there is a reduction in the average sleep. Implementing notifications to incentivise movement during the day, notifications to prepare before sleep, and information about the importance of sleep and sleep hygiene could help users to achieve an ideal amount of daily sleep.\n\n\n\n\n\n\n\n6. Top high-level recommendations\n\nImportant Note: These suggestions are made based on the above described dataset, which consists of a very small sample of 35 individuals from which we do not have information about their gender or age. Also, the data was only collected during two months. Further focusing on a dataset with a bigger sample, preferably focusing on female individuals (since that is Bellabeat’s target group) and including more months of data would give more accurate information.\n\n\nOne of the first observations is that not all users are consistent in using their tracker. I would suggest focusing on the positive characteristics of the Bellabeat Ivy+, such as being lightweight, hypoallergenic, discrete and stylish, waterproof, and its long lasting battery. All which makes it a tracker that can be worn daily without needing to take it off to go to bed or to shower, helping to record your daily measurements in a consistent manner and to get more accurate trends and recommendations. \nI would focus on women who might not be primarily focused on getting their exercise stats, but want to achieve a more mindful and balanced life and who like their accessories to reflect their lifestyle and personality.\nFor the Bellabeat app, I would make recommendations to promote ideas for their daily activities. Since most individuals do not get their needed daily 8,000 steps, I would incentivise walking even if it is in more casual scenarios. For people who don’t have time to exercise, promote taking the stairs instead of elevators, walking to nearby places, biking, etc. Since, less sedentarism is related to more (and maybe better) sleep, you can create short info capsules about why being more active during the day can help with having a good night sleep and the importance of sleep and its health benefits. You can also add reminders of when users, according to their preferences, should prepare to go to sleep in order to achieve a good night’s sleep. In general the app should both show individual stats, but also help incentivize the user to be more active giving good suggestions on how to do it."
  },
  {
    "objectID": "posts/Website Activity Dashboard /index.html",
    "href": "posts/Website Activity Dashboard /index.html",
    "title": "Website Activity Dasboard | PowerBI",
    "section": "",
    "text": "Website Activity Dashboard\nThis dashboard consists of website activity mock data from a fictional company called “Bicycle Empire”. This project was a technical exercise I submitted for a company where I was interviewing for a PowerBI analyst position. The exercise consisted on recreating all the main elements of the Reference Dashboard they sent me and to add a few of my own figures and elements. The dashboard in the right is the one made and submitted back to them.\nSince I don’t have the paid version of PowerBI, I share a few screenshots of the project.\n\n\n\n\n\n\n\n\n\nReference Dashboard\n\n\n\n\n\n\n\nSubmitted Dashboard\n\n\n\n\n\nThe requirements they gave consisted of the following KPI’s, calculated measures, and elements:\n\n1) Reach: calculated as the distinct count of Cust_id.\n2) Reach WoW: % change in reach (week over week).\na. If the number is negative, the number must automatically change to red. If it is positive it should be green.\nb. The number must be dynamic and always show the comparison of the last week selected vs the previous week.\n3) Impressions: count of cust_id where activity classification = Impression.\n4) Impressions WoW: % change in Impressions (week over week).\na. If the number is negative, the number must automatically change to red. If it is positive it should be green.\nb. The number must be dynamic and always show the comparison of the last week selected vs the previous week.\n5) Cumulative Reach: Ongoing accumulation of the Reach measure.\n6) Data through: Should show the latest date from the selected date range.\n\n\nAbout the Data\nThey sent me three csv files one with the Fact Table, and two Dimension tables for the Activity Classification and Brand Category. I loaded the files int to Power Bi and proceeded to inspect and clean the data.\nFact Table:\n\n\n\n  \n\n\n\nActivity Classification:\n\n\n\n  \n\n\n\nBrand Classification:\n\n\n\n  \n\n\n\n\n\nData Cleaning\nTo clean the data and for the ETL (Extract, Transform, and Load) process, the following instructions were given:\nETL:\n\nIdentify and remove unecessary columns to make the data load faster\nclean the brand name\nRemove “0” in Cust_ids\nCheck data types and headers\nRemove duplicated if necessary\n\nTable and relationships:\n\nCreate a calendar Table. Add a “Start of the Week” column and use it for the main chart\nCreate the necessary relationships between tables\n\nMeasures and Columns:\n\nCreate the necessary measures to show all the KPI’s in the reference dashboard.\nCreate a calculated column for tier in the category table using this information:\n\n\n\n\nCategory\nTier\n\n\n\n\nAndromeda\nTop\n\n\nWhirlpool\nTop\n\n\nTriangulum\nMiddle\n\n\nBlack Hawk\nMiddle\n\n\nCentaurus\nLow\n\n\nStarfish\nLow\n\n\n\nFrom all the instructions above I proceeded to clean the data in the Power Query Editor, deleting a few unnecessary columns and filtering out rows with 0 in the field Cust_id. I cleaned the Brand column by extracting the name before the “-” and replaced the name “Kronus” for “Cronus”. After checking that the data types of all columns were correct, i closed the editor and in the Table View I proceeded to create a Calendar Table and a table to store the calculated measurements. I also added a new column to the Brand Classification table to add the Tier Category using the formula SWITCH().\nIn the Model View I properly connected all the tables using a Star Schema and proceeded to do the calculations for the required KPI’s.\n\n\n\n\n\n\n\n\n\nCleaned Data\n\n\n\n\n\n\n\nCalendar Table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel View\n\n\n\n\n\n\n\nCalculated Measurments\n\n\n\n\n\n\n\nDashboard\nAfter finishing all the asked requirements, including all the filters, I reviewed the data to understand what other information could be useful to the hypothetical business. I already had metrics for Reach and Impressions, but decided to also add the metrics for Engagement, I also added bar charts to view all three metrics by Country and by Brand.\nAfter confirming all metrics were dynamic and all filters worked properly, the final step was to arrange the design for the KPI’s, select colors for my categories and add a background.\n\n\n\nFinal Dashboard\n\n\n\n\n\nFiltered Dashboard\n\n\nReach: How many individual people see your post or Ad.\nImpressions: How many times your content appears on someone’s screen.\nEngagement: The number of interactions your content received from users (likes, comments, shares, saves, etc.)."
  },
  {
    "objectID": "posts/Human Resources Dashboard/index.html",
    "href": "posts/Human Resources Dashboard/index.html",
    "title": "Human Resources Dashboard | Tableau",
    "section": "",
    "text": "This project consists of elaborating a highly interactive Tableau dashboard using mock data that mimics a Human Resources department type of data. The created dashboard is designed to follow specific requirements that could be asked by an HR department (described bellow), to achieve both a summary and a detailed view of the data. The dashboard is highly interactive, with filters and elaborated tooltips to obtain more information when hovering over each chart.\n\n\n\n\n\n\n\n\n\nSummary View\n\n\n\n\n\n\n\nDetailed View\n\n\n\n\n\n\n\nThe dataset consists of 8950 records that simulates a set of employee information typically found in HR systems, including: demographics, job details, salary, performance evaluations, and attrition data. The dataset was generated by prompting Chat-GPT for a python script, using the Faker library, with the following attributes:\n\nEmployee ID: A unique identifier.\nFirst Name: Randomly generated.\nLast Name: Randomly generated.\nGender: Randomly chosen with a 46% probability for ‘Female’ and a 54% probability for ‘Male’.\nState and City: Randomly assigned from a predefined list of states and their cities.\nHire Date: Randomly generated with custom probabilities for each year from 2015 to 2024.\nDepartment: Randomly chosen from a list of departments with specified probabilities.\nJob Title: Randomly selected based on the department, with specific probabilities for each job title within the department.\nEducation Level: Determined based on the job title, chosen from a predefined mapping of job titles to education levels.\nPerformance Rating: Randomly selected from ‘Excellent’, ‘Good’, ‘Satisfactory’, ‘Needs Improvement’ with specified probabilities.\nOvertime: Randomly chosen with a 30% probability for ‘Yes’ and a 70% probability for ‘No’.\nSalary: Generated based on the department and job title, within specific ranges.\nBirth Date: Generated based on age group distribution and job title requirements, ensuring consistency with the hire date.\nTermination Date: Assigned to a subset of employees (11.2% of the total) with specific probabilities for each year from 2015 to 2024, ensuring the termination date is at least 6 months after the hire date.\nAdjusted Salary: Calculated based on gender, education level, and age, applying specific multipliers and increments.\n\n\n\nThe used python script is the following:\nimport pandas as pd\nimport numpy as np\nfrom faker import Faker\nfrom datetime import datetime, timedelta\nimport random\n\n# Initialize Faker\nfake = Faker('en_US')\nFaker.seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# Configuration\nnum_records = 8950\n\n# States & Cities\nstates_cities = {\n    'New York': ['New York City', 'Buffalo', 'Rochester'],\n    'Virginia': ['Virginia Beach', 'Norfolk', 'Richmond'],\n    'Florida': ['Miami', 'Orlando', 'Tampa'],\n    'Illinois': ['Chicago', 'Aurora', 'Naperville'],\n    'Pennsylvania': ['Philadelphia', 'Pittsburgh', 'Allentown'],\n    'Ohio': ['Columbus', 'Cleveland', 'Cincinnati'],\n    'North Carolina': ['Charlotte', 'Raleigh', 'Greensboro'],\n    'Michigan': ['Detroit', 'Grand Rapids', 'Warren']\n}\nstates = list(states_cities.keys())\nstate_prob = [0.7, 0.02, 0.01, 0.03, 0.05, 0.03, 0.05, 0.11]\nassigned_states = np.random.choice(states, size=num_records, p=state_prob)\nassigned_cities = [np.random.choice(states_cities[state]) for state in assigned_states]\n\n# Departments & Jobtitles\ndepartments = ['HR', 'IT', 'Sales', 'Marketing', 'Finance', 'Operations', 'Customer Service']\ndepartments_prob = [0.02, 0.15, 0.21, 0.08, 0.05, 0.30, 0.19]\njobtitles = {\n    'HR': ['HR Manager', 'HR Coordinator', 'Recruiter', 'HR Assistant'],\n    'IT': ['IT Manager', 'Software Developer', 'System Administrator', 'IT Support Specialist'],\n    'Sales': ['Sales Manager', 'Sales Consultant', 'Sales Specialist', 'Sales Representative'],\n    'Marketing': ['Marketing Manager', 'SEO Specialist', 'Content Creator', 'Marketing Coordinator'],\n    'Finance': ['Finance Manager', 'Accountant', 'Financial Analyst', 'Accounts Payable Specialist'],\n    'Operations': ['Operations Manager', 'Operations Analyst', 'Logistics Coordinator', 'Inventory Specialist'],\n    'Customer Service': ['Customer Service Manager', 'Customer Service Representative', 'Support Specialist', 'Help Desk Technician']\n}\njobtitles_prob = {\n    'HR': [0.03, 0.3, 0.47, 0.2],  # HR Manager, HR Coordinator, Recruiter, HR Assistant\n    'IT': [0.02, 0.47, 0.2, 0.31],  # IT Manager, Software Developer, System Administrator, IT Support Specialist\n    'Sales': [0.03, 0.25, 0.32, 0.4],  # Sales Manager, Sales Consultant, Sales Specialist, Sales Representative\n    'Marketing': [0.04, 0.25, 0.41, 0.3],  # Marketing Manager, SEO Specialist, Content Creator, Marketing Coordinator\n    'Finance': [0.03, 0.37, 0.4, 0.2],  # Finance Manager, Accountant, Financial Analyst, Accounts Payable Specialist\n    'Operations': [0.02, 0.2, 0.4, 0.38],  # Operations Manager, Operations Analyst, Logistics Coordinator, Inventory Specialist\n    'Customer Service': [0.04, 0.3, 0.38, 0.28]  # Customer Service Manager, Customer Service Representative, Support Specialist, Help Desk Technician\n}\n\n# Educations\neducations = ['High School', \"Bachelor\", \"Master\", 'PhD']\n\neducation_mapping = {\n    'HR Manager': [\"Master\", \"PhD\"],\n    'HR Coordinator': [\"Bachelor\", \"Master\"],\n    'Recruiter': [\"High School\", \"Bachelor\"],\n    'HR Assistant': [\"High School\", \"Bachelor\"],\n    'IT Manager': [\"PhD\", \"Master\"],\n    'Software Developer': [\"Bachelor\", \"Master\"],\n    'System Administrator': [\"Bachelor\", \"Master\"],\n    'IT Support Specialist': [\"High School\", \"Bachelor\"],\n    'Sales Manager': [\"Master\",\"PhD\"],\n    'Sales Consultant': [\"Bachelor\", \"Master\", \"PhD\"],\n    'Sales Specialist': [\"Bachelor\", \"Master\", \"PhD\"],\n    'Sales Representative': [\"Bachelor\"],\n    'Marketing Manager': [\"Bachelor\", \"Master\",\"PhD\"],\n    'SEO Specialist': [\"High School\", \"Bachelor\"],\n    'Content Creator': [\"High School\", \"Bachelor\"],\n    'Marketing Coordinator': [\"Bachelor\"],\n    'Finance Manager': [\"Master\", \"PhD\"],\n    'Accountant': [\"Bachelor\"],\n    'Financial Analyst': [\"Bachelor\", \"Master\", \"PhD\"],\n    'Accounts Payable Specialist': [\"Bachelor\"],\n    'Operations Manager': [\"Bachelor\", \"Master\"],\n    'Operations Analyst': [\"Bachelor\", \"Master\"],\n    'Logistics Coordinator': [\"Bachelor\"],\n    'Inventory Specialist': [\"High School\", \"Bachelor\"],\n    'Customer Service Manager': [\"Bachelor\", \"Master\", \"PhD\"],\n    'Customer Service Representative': [\"High School\", \"Bachelor\"],\n    'Support Specialist': [\"High School\", \"Bachelor\"],\n    'Customer Success Manager': [\"Bachelor\", \"Master\", \"PhD\"],\n    'Help Desk Technician': [\"High School\", \"Bachelor\"]\n}\n\n# Hiring Date\n# Define custom probability weights for each year\nyear_weights = {\n    2015: 5,   # 15% probability\n    2016: 8,   # 15% probability\n    2017: 17,   # 20% probability\n    2018: 9,  # 15% probability\n    2019: 10,  # 10% probability\n    2020: 11,  # 10% probability\n    2021: 5,  # 8% probability\n    2022: 12,  # 5% probability\n    2023: 14,  # 2% probability\n    2024: 9   # 2% probability\n}\n\n\n# Generate a random date based on custom probabilities\ndef generate_custom_date(year_weights):\n    year = random.choices(list(year_weights.keys()), weights=list(year_weights.values()))[0]\n    month = random.randint(1, 12)\n    day = random.randint(1, 28)  # Assuming all months have 28 days for simplicity\n    return fake.date_time_between(start_date=datetime(year, 1, 1), end_date=datetime(year, 12, 31))\n\ndef generate_salary(department, job_title):\n    salary_dict = {\n            'HR': {\n                'HR Manager': np.random.randint(60000, 90000),\n                'HR Coordinator': np.random.randint(50000, 60000),\n                'Recruiter': np.random.randint(50000, 70000),\n                'HR Assistant': np.random.randint(50000, 60000)\n            },\n            'IT': {\n                'IT Manager': np.random.randint(80000, 120000),\n                'Software Developer': np.random.randint(70000, 95000),\n                'System Administrator': np.random.randint(60000, 90000),\n                'IT Support Specialist': np.random.randint(50000, 60000)\n            },\n            'Sales': {\n                'Sales Manager': np.random.randint(70000, 110000),\n                'Sales Consultant': np.random.randint(60000, 90000),\n                'Sales Specialist': np.random.randint(50000, 80000),\n                'Sales Representative': np.random.randint(50000, 70000)\n            },\n            'Marketing': {\n                'Marketing Manager': np.random.randint(70000, 100000),\n                'SEO Specialist': np.random.randint(50000, 80000),\n                'Content Creator': np.random.randint(50000, 60000),\n                'Marketing Coordinator': np.random.randint(50000, 70000)\n            },\n            'Finance': {\n                'Finance Manager': np.random.randint(80000, 120000),\n                'Accountant': np.random.randint(50000, 80000),\n                'Financial Analyst': np.random.randint(60000, 90000),\n                'Accounts Payable Specialist': np.random.randint(50000, 60000)\n            },\n            'Operations': {\n                'Operations Manager': np.random.randint(70000, 100000),\n                'Operations Analyst': np.random.randint(50000, 80000),\n                'Logistics Coordinator': np.random.randint(50000, 60000),\n                'Inventory Specialist': np.random.randint(50000, 60000)\n            },\n            'Customer Service': {\n                'Customer Service Manager': np.random.randint(60000, 90000),\n                'Customer Service Representative': np.random.randint(50000, 60000),\n                'Support Specialist': np.random.randint(50000, 60000),\n                'Help Desk Technician': np.random.randint(50000, 80000)\n            }\n        }\n    return salary_dict[department][job_title]\n\n# Generate the dataset\ndata = []\n\nfor _ in range(num_records):\n    employee_id = f\"00-{random.randint(10000000, 99999999)}\"\n    first_name = fake.first_name()\n    last_name = fake.last_name()\n    gender = np.random.choice(['Female', 'Male'], p=[0.46, 0.54])\n    state = np.random.choice(states, p=state_prob)\n    city = np.random.choice(states_cities[state])\n    hiredate = generate_custom_date(year_weights)\n      #termdate\n    department = np.random.choice(departments, p=departments_prob)\n    job_title  = np.random.choice(jobtitles[department], p=jobtitles_prob[department])\n    education_level = np.random.choice(education_mapping[job_title])\n    performance_rating = np.random.choice(['Excellent', 'Good', 'Satisfactory', 'Needs Improvement'], p=[0.12, 0.5, 0.3, 0.08])\n    overtime = np.random.choice(['Yes', 'No'], p=[0.3, 0.7])\n    salary = generate_salary(department, job_title)\n\n    data.append([\n        employee_id,\n        first_name,\n        last_name,\n        gender,\n        state,\n        city,\n        hiredate,\n        department,\n        job_title,\n        education_level,\n        salary,\n        performance_rating,\n        overtime\n    ])\n\n## Create DataFrame\ncolumns = [\n     'employee_id',\n     'first_name',\n     'last_name',\n     'gender',\n     'state',\n     'city',\n     'hiredate',\n     'department',\n     'job_title',\n     'education_level',\n     'salary',\n     'performance_rating',\n     'overtime'\n    ]\n\n\ndf = pd.DataFrame(data, columns=columns)\n\n# Add Birthdate\ndef generate_birthdate(row):\n    age_distribution = {\n        'under_25': 0.11,\n        '25_34': 0.25,\n        '35_44': 0.31,\n        '45_54': 0.24,\n        'over_55': 0.09\n    }\n    age_groups = list(age_distribution.keys())\n    age_probs = list(age_distribution.values())\n    age_group = np.random.choice(age_groups, p=age_probs)\n\n    if any('Manager' in title for title in row['job_title']):\n        age = np.random.randint(30, 65)\n    elif row['education_level'] == 'PhD':\n        age = np.random.randint(27, 65)\n    elif age_group == 'under_25':\n         age = np.random.randint(20, 25)\n    elif age_group == '25_34':\n        age = np.random.randint(25, 35)\n    elif age_group == '35_44':\n        age = np.random.randint(35, 45)\n    elif age_group == '45_54':\n        age = np.random.randint(45, 55)\n    else:\n        age = np.random.randint(56, 65)\n\n    birthdate = fake.date_of_birth(minimum_age=age, maximum_age=age)\n    return birthdate\n\n# Apply the function to generate birthdates\ndf['birthdate'] = df.apply(generate_birthdate, axis=1)\n\n# Terminations\n# Define termination distribution\nyear_weights = {\n    2015: 5,\n    2016: 7,\n    2017: 10,\n    2018: 12,\n    2019: 9,\n    2020: 10,\n    2021: 20,\n    2022: 10,\n    2023: 7,\n    2024: 10\n}\n\n# Calculate the total number of terminated employees\ntotal_employees = num_records\ntermination_percentage = 0.112  # 11.2%\ntotal_terminated = int(total_employees * termination_percentage)\n\n# Generate termination dates based on distribution\ntermination_dates = []\nfor year, weight in year_weights.items():\n    num_terminations = int(total_terminated * (weight / 100))\n    termination_dates.extend([year] * num_terminations)\n\n# Randomly shuffle the termination dates\nrandom.shuffle(termination_dates)\n\n# Assign termination dates to terminated employees\nterminated_indices = df.index[:total_terminated]\nfor i, year in enumerate(termination_dates[:total_terminated]):\n    df.at[terminated_indices[i], 'termdate'] = datetime(year, 1, 1) + timedelta(days=random.randint(0, 365))\n\n\n# Assign None to termdate for employees who are not terminated\ndf['termdate'] = df['termdate'].where(df['termdate'].notnull(), None)\n\n# Ensure termdate is at least 6 months after hiredat\ndf['termdate'] = df.apply(lambda row: row['hiredate'] + timedelta(days=180) if row['termdate'] and row['termdate'] &lt; row['hiredate'] + timedelta(days=180) else row['termdate'], axis=1)\n\neducation_multiplier = {\n    'High School': {'Male': 1.03, 'Female': 1.0},\n    \"Bachelor\": {'Male': 1.115, 'Female': 1.0},\n    \"Master\": {'Male': 1.0, 'Female': 1.07},\n    'PhD': {'Male': 1.0, 'Female': 1.17}\n}\n\n\n# Function to calculate age from birthdate\ndef calculate_age(birthdate):\n    today = pd.Timestamp('today')\n    age = today.year - birthdate.year - ((today.month, today.day) &lt; (birthdate.month, birthdate.day))\n    return age\n\n# Function to calculate the adjusted salary\ndef calculate_adjusted_salary(row):\n    base_salary = row['salary']\n    gender = row['gender']\n    education = row['education_level']\n    age = calculate_age(row['birthdate'])\n\n    # Apply education multiplier\n    multiplier = education_multiplier.get(education, {}).get(gender, 1.0)\n    adjusted_salary = base_salary * multiplier\n\n    # Apply age increment (between 0.1% and 0.3% per year of age)\n    age_increment = 1 + np.random.uniform(0.001, 0.003) * age\n    adjusted_salary *= age_increment\n\n    # Ensure the adjusted salary is not lower than the base salary\n    adjusted_salary = max(adjusted_salary, base_salary)\n\n    # Round the adjusted salary to the nearest integer\n    return round(adjusted_salary)\n\n# Apply the function to the DataFrame\ndf['salary'] = df.apply(calculate_adjusted_salary, axis=1)\n\n# Convert 'hiredate' and 'birthdate' to datetime\ndf['hiredate'] = pd.to_datetime(df['hiredate']).dt.date\ndf['birthdate'] = pd.to_datetime(df['birthdate']).dt.date\ndf['termdate'] = pd.to_datetime(df['termdate']).dt.date\n\nprint(df)\n\n# Save to CSV\ndf.to_csv('HumanResources.csv', index=False)\nThe file HumanResources.csv was imported into Tableau Public to make the visualizations.\n\n\n\n\nThe dashboard was designed in order to provide summary views for high-level insights and detailed employee records for in-depth analysis according to the following instructions:\n\n\nThe summary view should be divided into three main sections: Overview, Demographics, and Income Analysis.\n\n\nThe Overview section should provide a snapshot of the overall HR metrics, including:\n\nDisplay the total number of hired employees, active employees, and terminated employees.\nVisualize the total number of hired and terminated employees over the years.\nPresent a breakdown of total employees by department and job titles.\nCompare total employees between headquarters (HQ) and branches (New York is the HQ)\nShow the distribution of employees by city and state.\n\n\n\n\n\nThe Demographics section should offer insights into the composition of the workforce, including:\n\nPresent the gender ratio in the company.\nVisualize the distribution of employees across age groups and education levels.\nShow the total number of employees within each age group.\nShow the total number of employees within each education level.\nPresent the correlation between employees’s educational backgrounds and their performance ratings.\n\n\n\n\nThe income analysis section should focus on salary-related metrics, including:\n\nCompare salaries across different education levels for both genders to identify any discrepancies or patterns.\nPresent how the age correlate with the salary for employees in each department.\n\n\n\n\n\n\nProvide a comprehensive list of all employees with necessary information such as name, department, position, gender, age, education, and salary.\nUsers should be able to filter the list based on any of the available columns.\n\nThe dashboard has a navigation panel (left side) with buttons to change between the Summary and the Employee Records view. Both views have interactive filters that can be opened with buttons and by clicking on the charts. You can hover over the graphs to get more information about the each metric. Several calculated fields were required to make the different visualizations, such as the “Hired” or “Terminated” status, the length of hire, age groups, etc.\nIn regards to the aesthetic design of the dashboard, the final look was achieved by using a background image to get the rounded corners and the color fading of each panel. I used Pixelmator Pro, but it can be done with any other similar software (i.e. Figma, Illustrator, Inkscape, etc.).\n\nTo get a better view of the dashboard you can click the “full screen” button on the bottom right corner.\n\n                    \n\n*This project was done with the help of an amazing tutorial done by the YouTube channel DataWithBaraa."
  },
  {
    "objectID": "posts/Human Resources Dashboard/index.html#about-the-data",
    "href": "posts/Human Resources Dashboard/index.html#about-the-data",
    "title": "Human Resources Dashboard | Tableau",
    "section": "",
    "text": "The dataset consists of 8950 records that simulates a set of employee information typically found in HR systems, including: demographics, job details, salary, performance evaluations, and attrition data. The dataset was generated by prompting Chat-GPT for a python script, using the Faker library, with the following attributes:\n\nEmployee ID: A unique identifier.\nFirst Name: Randomly generated.\nLast Name: Randomly generated.\nGender: Randomly chosen with a 46% probability for ‘Female’ and a 54% probability for ‘Male’.\nState and City: Randomly assigned from a predefined list of states and their cities.\nHire Date: Randomly generated with custom probabilities for each year from 2015 to 2024.\nDepartment: Randomly chosen from a list of departments with specified probabilities.\nJob Title: Randomly selected based on the department, with specific probabilities for each job title within the department.\nEducation Level: Determined based on the job title, chosen from a predefined mapping of job titles to education levels.\nPerformance Rating: Randomly selected from ‘Excellent’, ‘Good’, ‘Satisfactory’, ‘Needs Improvement’ with specified probabilities.\nOvertime: Randomly chosen with a 30% probability for ‘Yes’ and a 70% probability for ‘No’.\nSalary: Generated based on the department and job title, within specific ranges.\nBirth Date: Generated based on age group distribution and job title requirements, ensuring consistency with the hire date.\nTermination Date: Assigned to a subset of employees (11.2% of the total) with specific probabilities for each year from 2015 to 2024, ensuring the termination date is at least 6 months after the hire date.\nAdjusted Salary: Calculated based on gender, education level, and age, applying specific multipliers and increments.\n\n\n\nThe used python script is the following:\nimport pandas as pd\nimport numpy as np\nfrom faker import Faker\nfrom datetime import datetime, timedelta\nimport random\n\n# Initialize Faker\nfake = Faker('en_US')\nFaker.seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# Configuration\nnum_records = 8950\n\n# States & Cities\nstates_cities = {\n    'New York': ['New York City', 'Buffalo', 'Rochester'],\n    'Virginia': ['Virginia Beach', 'Norfolk', 'Richmond'],\n    'Florida': ['Miami', 'Orlando', 'Tampa'],\n    'Illinois': ['Chicago', 'Aurora', 'Naperville'],\n    'Pennsylvania': ['Philadelphia', 'Pittsburgh', 'Allentown'],\n    'Ohio': ['Columbus', 'Cleveland', 'Cincinnati'],\n    'North Carolina': ['Charlotte', 'Raleigh', 'Greensboro'],\n    'Michigan': ['Detroit', 'Grand Rapids', 'Warren']\n}\nstates = list(states_cities.keys())\nstate_prob = [0.7, 0.02, 0.01, 0.03, 0.05, 0.03, 0.05, 0.11]\nassigned_states = np.random.choice(states, size=num_records, p=state_prob)\nassigned_cities = [np.random.choice(states_cities[state]) for state in assigned_states]\n\n# Departments & Jobtitles\ndepartments = ['HR', 'IT', 'Sales', 'Marketing', 'Finance', 'Operations', 'Customer Service']\ndepartments_prob = [0.02, 0.15, 0.21, 0.08, 0.05, 0.30, 0.19]\njobtitles = {\n    'HR': ['HR Manager', 'HR Coordinator', 'Recruiter', 'HR Assistant'],\n    'IT': ['IT Manager', 'Software Developer', 'System Administrator', 'IT Support Specialist'],\n    'Sales': ['Sales Manager', 'Sales Consultant', 'Sales Specialist', 'Sales Representative'],\n    'Marketing': ['Marketing Manager', 'SEO Specialist', 'Content Creator', 'Marketing Coordinator'],\n    'Finance': ['Finance Manager', 'Accountant', 'Financial Analyst', 'Accounts Payable Specialist'],\n    'Operations': ['Operations Manager', 'Operations Analyst', 'Logistics Coordinator', 'Inventory Specialist'],\n    'Customer Service': ['Customer Service Manager', 'Customer Service Representative', 'Support Specialist', 'Help Desk Technician']\n}\njobtitles_prob = {\n    'HR': [0.03, 0.3, 0.47, 0.2],  # HR Manager, HR Coordinator, Recruiter, HR Assistant\n    'IT': [0.02, 0.47, 0.2, 0.31],  # IT Manager, Software Developer, System Administrator, IT Support Specialist\n    'Sales': [0.03, 0.25, 0.32, 0.4],  # Sales Manager, Sales Consultant, Sales Specialist, Sales Representative\n    'Marketing': [0.04, 0.25, 0.41, 0.3],  # Marketing Manager, SEO Specialist, Content Creator, Marketing Coordinator\n    'Finance': [0.03, 0.37, 0.4, 0.2],  # Finance Manager, Accountant, Financial Analyst, Accounts Payable Specialist\n    'Operations': [0.02, 0.2, 0.4, 0.38],  # Operations Manager, Operations Analyst, Logistics Coordinator, Inventory Specialist\n    'Customer Service': [0.04, 0.3, 0.38, 0.28]  # Customer Service Manager, Customer Service Representative, Support Specialist, Help Desk Technician\n}\n\n# Educations\neducations = ['High School', \"Bachelor\", \"Master\", 'PhD']\n\neducation_mapping = {\n    'HR Manager': [\"Master\", \"PhD\"],\n    'HR Coordinator': [\"Bachelor\", \"Master\"],\n    'Recruiter': [\"High School\", \"Bachelor\"],\n    'HR Assistant': [\"High School\", \"Bachelor\"],\n    'IT Manager': [\"PhD\", \"Master\"],\n    'Software Developer': [\"Bachelor\", \"Master\"],\n    'System Administrator': [\"Bachelor\", \"Master\"],\n    'IT Support Specialist': [\"High School\", \"Bachelor\"],\n    'Sales Manager': [\"Master\",\"PhD\"],\n    'Sales Consultant': [\"Bachelor\", \"Master\", \"PhD\"],\n    'Sales Specialist': [\"Bachelor\", \"Master\", \"PhD\"],\n    'Sales Representative': [\"Bachelor\"],\n    'Marketing Manager': [\"Bachelor\", \"Master\",\"PhD\"],\n    'SEO Specialist': [\"High School\", \"Bachelor\"],\n    'Content Creator': [\"High School\", \"Bachelor\"],\n    'Marketing Coordinator': [\"Bachelor\"],\n    'Finance Manager': [\"Master\", \"PhD\"],\n    'Accountant': [\"Bachelor\"],\n    'Financial Analyst': [\"Bachelor\", \"Master\", \"PhD\"],\n    'Accounts Payable Specialist': [\"Bachelor\"],\n    'Operations Manager': [\"Bachelor\", \"Master\"],\n    'Operations Analyst': [\"Bachelor\", \"Master\"],\n    'Logistics Coordinator': [\"Bachelor\"],\n    'Inventory Specialist': [\"High School\", \"Bachelor\"],\n    'Customer Service Manager': [\"Bachelor\", \"Master\", \"PhD\"],\n    'Customer Service Representative': [\"High School\", \"Bachelor\"],\n    'Support Specialist': [\"High School\", \"Bachelor\"],\n    'Customer Success Manager': [\"Bachelor\", \"Master\", \"PhD\"],\n    'Help Desk Technician': [\"High School\", \"Bachelor\"]\n}\n\n# Hiring Date\n# Define custom probability weights for each year\nyear_weights = {\n    2015: 5,   # 15% probability\n    2016: 8,   # 15% probability\n    2017: 17,   # 20% probability\n    2018: 9,  # 15% probability\n    2019: 10,  # 10% probability\n    2020: 11,  # 10% probability\n    2021: 5,  # 8% probability\n    2022: 12,  # 5% probability\n    2023: 14,  # 2% probability\n    2024: 9   # 2% probability\n}\n\n\n# Generate a random date based on custom probabilities\ndef generate_custom_date(year_weights):\n    year = random.choices(list(year_weights.keys()), weights=list(year_weights.values()))[0]\n    month = random.randint(1, 12)\n    day = random.randint(1, 28)  # Assuming all months have 28 days for simplicity\n    return fake.date_time_between(start_date=datetime(year, 1, 1), end_date=datetime(year, 12, 31))\n\ndef generate_salary(department, job_title):\n    salary_dict = {\n            'HR': {\n                'HR Manager': np.random.randint(60000, 90000),\n                'HR Coordinator': np.random.randint(50000, 60000),\n                'Recruiter': np.random.randint(50000, 70000),\n                'HR Assistant': np.random.randint(50000, 60000)\n            },\n            'IT': {\n                'IT Manager': np.random.randint(80000, 120000),\n                'Software Developer': np.random.randint(70000, 95000),\n                'System Administrator': np.random.randint(60000, 90000),\n                'IT Support Specialist': np.random.randint(50000, 60000)\n            },\n            'Sales': {\n                'Sales Manager': np.random.randint(70000, 110000),\n                'Sales Consultant': np.random.randint(60000, 90000),\n                'Sales Specialist': np.random.randint(50000, 80000),\n                'Sales Representative': np.random.randint(50000, 70000)\n            },\n            'Marketing': {\n                'Marketing Manager': np.random.randint(70000, 100000),\n                'SEO Specialist': np.random.randint(50000, 80000),\n                'Content Creator': np.random.randint(50000, 60000),\n                'Marketing Coordinator': np.random.randint(50000, 70000)\n            },\n            'Finance': {\n                'Finance Manager': np.random.randint(80000, 120000),\n                'Accountant': np.random.randint(50000, 80000),\n                'Financial Analyst': np.random.randint(60000, 90000),\n                'Accounts Payable Specialist': np.random.randint(50000, 60000)\n            },\n            'Operations': {\n                'Operations Manager': np.random.randint(70000, 100000),\n                'Operations Analyst': np.random.randint(50000, 80000),\n                'Logistics Coordinator': np.random.randint(50000, 60000),\n                'Inventory Specialist': np.random.randint(50000, 60000)\n            },\n            'Customer Service': {\n                'Customer Service Manager': np.random.randint(60000, 90000),\n                'Customer Service Representative': np.random.randint(50000, 60000),\n                'Support Specialist': np.random.randint(50000, 60000),\n                'Help Desk Technician': np.random.randint(50000, 80000)\n            }\n        }\n    return salary_dict[department][job_title]\n\n# Generate the dataset\ndata = []\n\nfor _ in range(num_records):\n    employee_id = f\"00-{random.randint(10000000, 99999999)}\"\n    first_name = fake.first_name()\n    last_name = fake.last_name()\n    gender = np.random.choice(['Female', 'Male'], p=[0.46, 0.54])\n    state = np.random.choice(states, p=state_prob)\n    city = np.random.choice(states_cities[state])\n    hiredate = generate_custom_date(year_weights)\n      #termdate\n    department = np.random.choice(departments, p=departments_prob)\n    job_title  = np.random.choice(jobtitles[department], p=jobtitles_prob[department])\n    education_level = np.random.choice(education_mapping[job_title])\n    performance_rating = np.random.choice(['Excellent', 'Good', 'Satisfactory', 'Needs Improvement'], p=[0.12, 0.5, 0.3, 0.08])\n    overtime = np.random.choice(['Yes', 'No'], p=[0.3, 0.7])\n    salary = generate_salary(department, job_title)\n\n    data.append([\n        employee_id,\n        first_name,\n        last_name,\n        gender,\n        state,\n        city,\n        hiredate,\n        department,\n        job_title,\n        education_level,\n        salary,\n        performance_rating,\n        overtime\n    ])\n\n## Create DataFrame\ncolumns = [\n     'employee_id',\n     'first_name',\n     'last_name',\n     'gender',\n     'state',\n     'city',\n     'hiredate',\n     'department',\n     'job_title',\n     'education_level',\n     'salary',\n     'performance_rating',\n     'overtime'\n    ]\n\n\ndf = pd.DataFrame(data, columns=columns)\n\n# Add Birthdate\ndef generate_birthdate(row):\n    age_distribution = {\n        'under_25': 0.11,\n        '25_34': 0.25,\n        '35_44': 0.31,\n        '45_54': 0.24,\n        'over_55': 0.09\n    }\n    age_groups = list(age_distribution.keys())\n    age_probs = list(age_distribution.values())\n    age_group = np.random.choice(age_groups, p=age_probs)\n\n    if any('Manager' in title for title in row['job_title']):\n        age = np.random.randint(30, 65)\n    elif row['education_level'] == 'PhD':\n        age = np.random.randint(27, 65)\n    elif age_group == 'under_25':\n         age = np.random.randint(20, 25)\n    elif age_group == '25_34':\n        age = np.random.randint(25, 35)\n    elif age_group == '35_44':\n        age = np.random.randint(35, 45)\n    elif age_group == '45_54':\n        age = np.random.randint(45, 55)\n    else:\n        age = np.random.randint(56, 65)\n\n    birthdate = fake.date_of_birth(minimum_age=age, maximum_age=age)\n    return birthdate\n\n# Apply the function to generate birthdates\ndf['birthdate'] = df.apply(generate_birthdate, axis=1)\n\n# Terminations\n# Define termination distribution\nyear_weights = {\n    2015: 5,\n    2016: 7,\n    2017: 10,\n    2018: 12,\n    2019: 9,\n    2020: 10,\n    2021: 20,\n    2022: 10,\n    2023: 7,\n    2024: 10\n}\n\n# Calculate the total number of terminated employees\ntotal_employees = num_records\ntermination_percentage = 0.112  # 11.2%\ntotal_terminated = int(total_employees * termination_percentage)\n\n# Generate termination dates based on distribution\ntermination_dates = []\nfor year, weight in year_weights.items():\n    num_terminations = int(total_terminated * (weight / 100))\n    termination_dates.extend([year] * num_terminations)\n\n# Randomly shuffle the termination dates\nrandom.shuffle(termination_dates)\n\n# Assign termination dates to terminated employees\nterminated_indices = df.index[:total_terminated]\nfor i, year in enumerate(termination_dates[:total_terminated]):\n    df.at[terminated_indices[i], 'termdate'] = datetime(year, 1, 1) + timedelta(days=random.randint(0, 365))\n\n\n# Assign None to termdate for employees who are not terminated\ndf['termdate'] = df['termdate'].where(df['termdate'].notnull(), None)\n\n# Ensure termdate is at least 6 months after hiredat\ndf['termdate'] = df.apply(lambda row: row['hiredate'] + timedelta(days=180) if row['termdate'] and row['termdate'] &lt; row['hiredate'] + timedelta(days=180) else row['termdate'], axis=1)\n\neducation_multiplier = {\n    'High School': {'Male': 1.03, 'Female': 1.0},\n    \"Bachelor\": {'Male': 1.115, 'Female': 1.0},\n    \"Master\": {'Male': 1.0, 'Female': 1.07},\n    'PhD': {'Male': 1.0, 'Female': 1.17}\n}\n\n\n# Function to calculate age from birthdate\ndef calculate_age(birthdate):\n    today = pd.Timestamp('today')\n    age = today.year - birthdate.year - ((today.month, today.day) &lt; (birthdate.month, birthdate.day))\n    return age\n\n# Function to calculate the adjusted salary\ndef calculate_adjusted_salary(row):\n    base_salary = row['salary']\n    gender = row['gender']\n    education = row['education_level']\n    age = calculate_age(row['birthdate'])\n\n    # Apply education multiplier\n    multiplier = education_multiplier.get(education, {}).get(gender, 1.0)\n    adjusted_salary = base_salary * multiplier\n\n    # Apply age increment (between 0.1% and 0.3% per year of age)\n    age_increment = 1 + np.random.uniform(0.001, 0.003) * age\n    adjusted_salary *= age_increment\n\n    # Ensure the adjusted salary is not lower than the base salary\n    adjusted_salary = max(adjusted_salary, base_salary)\n\n    # Round the adjusted salary to the nearest integer\n    return round(adjusted_salary)\n\n# Apply the function to the DataFrame\ndf['salary'] = df.apply(calculate_adjusted_salary, axis=1)\n\n# Convert 'hiredate' and 'birthdate' to datetime\ndf['hiredate'] = pd.to_datetime(df['hiredate']).dt.date\ndf['birthdate'] = pd.to_datetime(df['birthdate']).dt.date\ndf['termdate'] = pd.to_datetime(df['termdate']).dt.date\n\nprint(df)\n\n# Save to CSV\ndf.to_csv('HumanResources.csv', index=False)\nThe file HumanResources.csv was imported into Tableau Public to make the visualizations."
  },
  {
    "objectID": "posts/Human Resources Dashboard/index.html#dashboard",
    "href": "posts/Human Resources Dashboard/index.html#dashboard",
    "title": "Human Resources Dashboard | Tableau",
    "section": "",
    "text": "The dashboard was designed in order to provide summary views for high-level insights and detailed employee records for in-depth analysis according to the following instructions:\n\n\nThe summary view should be divided into three main sections: Overview, Demographics, and Income Analysis.\n\n\nThe Overview section should provide a snapshot of the overall HR metrics, including:\n\nDisplay the total number of hired employees, active employees, and terminated employees.\nVisualize the total number of hired and terminated employees over the years.\nPresent a breakdown of total employees by department and job titles.\nCompare total employees between headquarters (HQ) and branches (New York is the HQ)\nShow the distribution of employees by city and state.\n\n\n\n\n\nThe Demographics section should offer insights into the composition of the workforce, including:\n\nPresent the gender ratio in the company.\nVisualize the distribution of employees across age groups and education levels.\nShow the total number of employees within each age group.\nShow the total number of employees within each education level.\nPresent the correlation between employees’s educational backgrounds and their performance ratings.\n\n\n\n\nThe income analysis section should focus on salary-related metrics, including:\n\nCompare salaries across different education levels for both genders to identify any discrepancies or patterns.\nPresent how the age correlate with the salary for employees in each department.\n\n\n\n\n\n\nProvide a comprehensive list of all employees with necessary information such as name, department, position, gender, age, education, and salary.\nUsers should be able to filter the list based on any of the available columns.\n\nThe dashboard has a navigation panel (left side) with buttons to change between the Summary and the Employee Records view. Both views have interactive filters that can be opened with buttons and by clicking on the charts. You can hover over the graphs to get more information about the each metric. Several calculated fields were required to make the different visualizations, such as the “Hired” or “Terminated” status, the length of hire, age groups, etc.\nIn regards to the aesthetic design of the dashboard, the final look was achieved by using a background image to get the rounded corners and the color fading of each panel. I used Pixelmator Pro, but it can be done with any other similar software (i.e. Figma, Illustrator, Inkscape, etc.).\n\nTo get a better view of the dashboard you can click the “full screen” button on the bottom right corner.\n\n                    \n\n*This project was done with the help of an amazing tutorial done by the YouTube channel DataWithBaraa."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Margarita’s resume",
    "section": "",
    "text": "download Margarita’s resume"
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "Margarita’s resume",
    "section": "Skills",
    "text": "Skills\n\nRelational databases and SQL\nR (dplyr, ggplot2, tidyr, gt)\nExcel/Google Sheets (VLookup, Conditional formatting, Pivot Tables)\nParameterized reports\nTableau | Looker Studio | PowerBI\nData cleaning and wrangling with R\nCommunication and presentation skills\nStrong data visualization skills\nProblem solving\nAnalytical skills"
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Margarita’s resume",
    "section": "Experience",
    "text": "Experience\n\nCenter for Transdisciplinary Research in Psychology, UAEM. Mexico   Fellow Researcher\nJAN 2024 - PRESENT\n\nData Management & Analysis: Led the design, implementation, and maintenance of comprehensive databases for experimental data. Applied data cleaning protocols to ensure data integrity and accuracy. Performed in-depth statistical analysis and created insightful visualizations (using R,  ggplot2) to reveal trends and patterns in complex datasets.\nTraining & Knowledge Sharing: Developed and delivered workshops on statistics, programming in R, and data analysis techniques, empowering colleagues to enhance their research capabilities.\n\n\n\nMorelos Children’s Hospital, Mexico   Postdoctoral Researcher\nJUN  2020 - NOV  2023\n\nData Collection & Analysis: Designed and managed the implementation of patient surveys, defining key metrics and ensuring data quality. Collaborated with diverse healthcare teams to gather requirements and collect patient samples. Conducted comprehensive analysis of survey and patient sample data, utilizing R (ggplot2, gt) to create informative visualizations that effectively communicated research findings to stakeholders.\nHealth Education: Organized and presented informative talks on the health risks associated with viral diseases to raise awareness among the public.\n\n\n\nLeibniz Institute for Virology, Hamburg   Doctoral Researcher\nSEP 2012 - OCT 2018\n\nLarge-Scale Data Analysis: Designed and executed experiments involving large-scale sequencing data analysis of thousands of transcripts across diverse conditions. Employed bioinformatics tools to identify significant changes induced by viral infection, contributing to a deeper understanding of viral mechanisms.\nScientific Communication: Presented research findings at international conferences, showcasing the ability to effectively communicate complex data analysis to a scientific audience.\n\n\n\nBC & B  Law & Business, Mexico   Biotechnology Engineer\nJAN - MAY  2012\n\nPatent Analysis & Consultation: Provided expert consultation to national and international companies, guiding them through the patent application process under Mexican industrial property law. Conducted in-depth patent analysis within the chemistry, pharmaceuticals, and biotechnology sectors."
  },
  {
    "objectID": "resume.html#courses-certifications",
    "href": "resume.html#courses-certifications",
    "title": "Margarita’s resume",
    "section": "Courses & Certifications",
    "text": "Courses & Certifications\nGoogle Data Analytics Professional Certificate | Google, Coursera | 2024\nData Cleaning with R Course | R for the Rest of Us | 2022\nR Programming | Johns Hopkins University, Coursera | 2015"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Margarita’s resume",
    "section": "Education",
    "text": "Education\n\nHamburg University, Germany | Doctoral degree\nDoctor of Philosophy, Virology | Faculty of Mathematics, Informatics and  Natural Sciences.\nJUN 2012 - OCT  2018\n\n\nNational Autonomous University of Mexico, Mexico | Master’s Degree\nMasters in Biochemical Sciences | Biotechnology Institute.\nFEB 2009 - NOV 2011\n\n\nNational Autonomous University of Morelos, Mexico | Bachelor’s Degree\nBachelor of Science in Biochemistry and Molecular Biology | Faculty of Sciences.\nAUG 2004 - DEC 2008"
  },
  {
    "objectID": "posts/Dengue cases in México/index.html",
    "href": "posts/Dengue cases in México/index.html",
    "title": "Dengue Cases in Mexico",
    "section": "",
    "text": "Dengue cases in Mexico (2024)\nFor this analysis, the data wrangling was done with R using the {tidyverse} library of packages. To visualize the data I made an interactive dashboard in Tableau.\n\n1) About the data\nThe data for this project was downloaded from the Mexican Health Secretary database webpage. The collected data spans from the 1st of January to the 4th of June 2024. The day cases are reported is later than the start of symptoms date, so the monitoring of cases has a lag between the symptoms and when they are reported, which can be several days later.\nThree files were downloaded: 1) the first is a .csv file with the actual dengue data Database of Dengue in Mexico (even thought the title is not updated and it says 2020, after inspecting it, the file does indeed contain the 2024 data). 2) The second and 3) third are .xlsx files, and they are downloaded together in a folder. They contain the Dictionary and Catalog of Dengue. The catalog contains the keys for all the variables in the dengue database, and the dictionary contains the description of the variables.\n\n\n2) Data Wrangling\n\na) Importing the data\nFirst, I imported the “dengue_abierto.csv” file to R Studio as a tibble. I inspected the columns and rows, and found there are 78,184 observations and 28 variables. Most of the variables are coded in keys, which are translated later on.\n\nlibrary(tidyverse)\n(casos &lt;- read_csv(file = \"dengue_abierto.csv\"))\n\n\n  \n\n\n\n\n\nb) Selecting working variables\nI decided which variables I was interested in analyzing and selected those columns, to only focus on that data. The variables I selected contained the: ID, sex, age, state of residency, date of intial symptoms, type of patient, hemorrhagic cases, death cases, if they were sampled, PCR results, and status of the case.\n\n(selected_variables &lt;- casos %&gt;% \n          select(ID_REGISTRO,\n                 SEXO,\n                 EDAD_ANOS,\n                 ENTIDAD_RES,\n                 FECHA_SIGN_SINTOMAS,\n                 TIPO_PACIENTE,\n                 HEMORRAGICOS,\n                 DEFUNCION,\n                 DICTAMEN,\n                 TOMA_MUESTRA,\n                 RESULTADO_PCR,\n                 ESTATUS_CASO))\n\n\n\nc) Re-writing keys\nUsing the file “Catalogos_Dengue.xlsx” I renamed the keys on the working variables I selected, to understand the data set I was working with. As an example, keys (in numbers) were used to declare if a variable was negative or positive, the state of residency, the sex, etc.\n\n(rename_variables &lt;- selected_variables %&gt;% \n          mutate(SEXO = case_when(SEXO == 1 ~ \"Mujer\",\n                                  SEXO == 2 ~ \"Hombre\")) %&gt;% \n          mutate(ENTIDAD_RES = case_when(ENTIDAD_RES == \"01\" ~ \"Aguascalientes\",\n                                    ENTIDAD_RES == \"02\" ~ \"Baja California\",\n                                    ENTIDAD_RES == \"03\" ~ \"Baja California Sur\",\n                                    ENTIDAD_RES == \"04\" ~ \"Campeche\",\n                                    ENTIDAD_RES == \"05\" ~ \"Coahuila\",\n                                    ENTIDAD_RES == \"06\" ~ \"Colima\",\n                                    ENTIDAD_RES == \"07\" ~ \"Chiapas\",\n                                    ENTIDAD_RES == \"08\" ~ \"Chihuahua\",\n                                    ENTIDAD_RES == \"09\" ~ \"Ciudad de México\",\n                                    ENTIDAD_RES == \"10\" ~ \"Durango\",\n                                    ENTIDAD_RES == \"11\" ~ \"Guanajuato\",\n                                    ENTIDAD_RES == \"12\" ~ \"Guerrero\",\n                                    ENTIDAD_RES == \"13\" ~ \"Hidalgo\",\n                                    ENTIDAD_RES == \"14\" ~ \"Jalisco\",\n                                    ENTIDAD_RES == \"15\" ~ \"México\",\n                                    ENTIDAD_RES == \"16\" ~ \"Michoacán\",\n                                    ENTIDAD_RES == \"17\" ~ \"Morelos\",\n                                    ENTIDAD_RES == \"18\" ~ \"Nayarit\",\n                                    ENTIDAD_RES == \"19\" ~ \"Nuevo León\",\n                                    ENTIDAD_RES == \"20\" ~ \"Oaxaca\",\n                                    ENTIDAD_RES == \"21\" ~ \"Puebla\",\n                                    ENTIDAD_RES == \"22\" ~ \"Querétaro\",\n                                    ENTIDAD_RES == \"23\" ~ \"Quintana Roo\",\n                                    ENTIDAD_RES == \"24\" ~ \"San Luis Potosí\",\n                                    ENTIDAD_RES == \"25\" ~ \"Sinaloa\",\n                                    ENTIDAD_RES == \"26\" ~ \"Sonora\",\n                                    ENTIDAD_RES == \"27\" ~ \"Tabasco\",\n                                    ENTIDAD_RES == \"28\" ~ \"Tamaulipas\",\n                                    ENTIDAD_RES == \"29\" ~ \"Tlaxcala\",\n                                    ENTIDAD_RES == \"30\" ~ \"Veracruz\",\n                                    ENTIDAD_RES == \"31\" ~ \"Yucatán\",\n                                    ENTIDAD_RES == \"32\" ~ \"Zacatecas\",\n                                    ENTIDAD_RES == \"33\" ~ \"Otros paises\",\n                                    ENTIDAD_RES == \"34\" ~ \"Otros paises\",\n                                    ENTIDAD_RES == \"35\" ~ \"Otros paises\",\n                                    .default = as.character(ENTIDAD_RES))) %&gt;% \n           mutate(TIPO_PACIENTE = case_when(TIPO_PACIENTE == 1 ~ \"ambulatorio\",\n                                            TIPO_PACIENTE == 2 ~ \"hospitalizado\")) %&gt;% \n           mutate(HEMORRAGICOS = case_when(HEMORRAGICOS == 1 ~ \"hemorragico\",\n                                           HEMORRAGICOS == 2 ~ \"no hemorragico\")) %&gt;% \n           mutate(DEFUNCION = case_when(DEFUNCION == 1 ~ \"si\",\n                                        DEFUNCION == 2 ~ \"no\")) %&gt;% \n           mutate(DICTAMEN = case_when(DICTAMEN == 1 ~ \"dengue\",\n                                       DICTAMEN == 2 ~ \"chikungunya\",\n                                       DICTAMEN == 3 ~ \"negativo\",\n                                       DICTAMEN == 4 ~ \"en estudio\",\n                                       DICTAMEN == 5 ~ \"no aplica\")) %&gt;% \n           mutate(TOMA_MUESTRA = case_when(TOMA_MUESTRA == 1 ~ \"si\",\n                                           TOMA_MUESTRA == 2 ~ \"no\")) %&gt;% \n           mutate(RESULTADO_PCR = case_when(RESULTADO_PCR == 1 ~ \"DENV1\",\n                                            RESULTADO_PCR == 2 ~ \"DENV2\",\n                                            RESULTADO_PCR == 3 ~ \"DENV3\",\n                                            RESULTADO_PCR == 4 ~ \"DENV4\",\n                                            RESULTADO_PCR == 5 ~ \"sin serotipo aislado\")) %&gt;% \n           mutate(ESTATUS_CASO = case_when(ESTATUS_CASO == 1 ~ \"probable\",\n                                           ESTATUS_CASO == 2 ~ \"confirmado\",\n                                           ESTATUS_CASO == 3 ~ \"descartado\"))\n                   \n )\n\n\n  \n\n\n\n\n\nd) Filtering out negative cases\nAs I was only interested in working with the possible positive cases, I filtered the variable containing the “status of the case” to only keep the confirmed and the possible cases, and to filter out negative cases. Afterwards, I saved the cleaned and filterd data as a .csv file.\n\n(positive_cases &lt;- rename_variables %&gt;% \n          filter(ESTATUS_CASO != \"descartado\")) \n\n# write_csv(positive_cases,\n#           file = \"positive_dengue_cases.csv\")\n\n\n\n\n3) Data Visualization\nThe visualization was done in the Tableau Public desktop app. I imported the “positive_dengue_cases.csv” to the app and proceeded to explore the data and finally decided to show the data for daily cases (as a bar chart), the cases per state (as a map), and the isolated serotypes of the dengue virus, the number of hemorrhagic cases divided by sex, and the number of deaths divided on weather they were hospitalized or outpatient.\n\nImportant note: The shown dates of start of symptoms differ from the day cases are reported and there is a lag of several days between when cases start and when they get reported. Therefore, the effect of cases dropping in the last days is due to this lag, not to the actual drop in cases."
  },
  {
    "objectID": "posts/Mexico's Wildfires/index.html",
    "href": "posts/Mexico's Wildfires/index.html",
    "title": "Wildfires in Mexico (2024)",
    "section": "",
    "text": "The following Tableau Dashboard was made using the latest data about wildfires in Mexico from the National Concentration of Forest Fires in protected natural areas. The captured data spans from January to the first week of June 2024. To get a better view of the dashboard, click on the “full screen” button in the lower right corner.\nThe data is in Spanish, but it encompasses the number of wildfires per week, the damaged area, the states where the fires occurred in, the type of vegetation damaged by the fires, and possible causes for the fires. The dashboard allows you to filter by each of the variables by clicking, or filtering by a range of weeks on the upper bar."
  },
  {
    "objectID": "posts/UFO Sightings/index.html",
    "href": "posts/UFO Sightings/index.html",
    "title": "Reports of UFO Sightings | Looker Studio",
    "section": "",
    "text": "This analysis consists of a dashboard done in Google Looker Studio with a little bit of data wrangling using Google Sheets. The dashboard displays data about reports of undefined flying objects (UFOs) sightings, compiled by The National UFO Reporting Center (NUFOC).\n\n\nThe data set of UFO sightings was obtain from Anna Wolak’s Kaggle page. The data set I used is the scrubbed data, which is the NUFORC data that was geolocated, and time standardized by Sigmond Axel. This data set contains over 80,000 reports of UFO sightings over the last century, from 1910 to 2013. Newer reports from the last years can be found in the NUFOC database, but the data cannot be downloaded. However, a summary about each of the sightings is available, so if you are interested in any of them I recommend visiting their page.\n\n\n\nI uploaded the scrubbed.csv data data set to Google Sheets and did a little bit of data wrangling to change the column of country in order to spell the full names of the countries (using the IFS() function) needed to be displayed in further figures or tables. In Looker Studio I connected de Google sheet with the data and realized I needed the geo data with the coordinates to be in a single column so I added a calculated field with both coordinates separated by a comma (“latitude, longitude” using the function CONCAT()).\n\n\n\nThe bellow dashboard displays information about how many sightings have been reported, the main countries where reports come from, the top shapes of UFOs reported, and a map of the locations where they were observed. You can also see a time line of when the sightings occurred. The dashboard can be filter by country, shape or date range. To have a better look of the dashboard you can open it in the following link: Reports of UFO Sightings."
  },
  {
    "objectID": "posts/UFO Sightings/index.html#about-the-data",
    "href": "posts/UFO Sightings/index.html#about-the-data",
    "title": "Reports of UFO Sightings | Looker Studio",
    "section": "",
    "text": "The data set of UFO sightings was obtain from Anna Wolak’s Kaggle page. The data set I used is the scrubbed data, which is the NUFORC data that was geolocated, and time standardized by Sigmond Axel. This data set contains over 80,000 reports of UFO sightings over the last century, from 1910 to 2013. Newer reports from the last years can be found in the NUFOC database, but the data cannot be downloaded. However, a summary about each of the sightings is available, so if you are interested in any of them I recommend visiting their page."
  },
  {
    "objectID": "posts/UFO Sightings/index.html#data-wrangling",
    "href": "posts/UFO Sightings/index.html#data-wrangling",
    "title": "Reports of UFO Sightings | Looker Studio",
    "section": "",
    "text": "I uploaded the scrubbed.csv data data set to Google Sheets and did a little bit of data wrangling to change the column of country in order to spell the full names of the countries (using the IFS() function) needed to be displayed in further figures or tables. In Looker Studio I connected de Google sheet with the data and realized I needed the geo data with the coordinates to be in a single column so I added a calculated field with both coordinates separated by a comma (“latitude, longitude” using the function CONCAT())."
  },
  {
    "objectID": "posts/UFO Sightings/index.html#data-visualization",
    "href": "posts/UFO Sightings/index.html#data-visualization",
    "title": "Reports of UFO Sightings | Looker Studio",
    "section": "",
    "text": "The bellow dashboard displays information about how many sightings have been reported, the main countries where reports come from, the top shapes of UFOs reported, and a map of the locations where they were observed. You can also see a time line of when the sightings occurred. The dashboard can be filter by country, shape or date range. To have a better look of the dashboard you can open it in the following link: Reports of UFO Sightings."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hi, my name is Margarita, I am a data analyst with a strong foundation in research and hold a PhD in Molecular Virology. I have experience analyzing large datasets from questionnaires and experimental measurements. During my research, I honed my skills in statistical analysis, data visualization, project management, and problem-solving to tackle complex scientific questions using data. I’m proficient in spreadsheets, SQL, Tableau, Looker Studio, PowerBI and R. My passion lies in transforming raw data into insights that can drive informed decision-making."
  }
]